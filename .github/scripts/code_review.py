#!/usr/bin/env python3

import os
import sys
import subprocess
from pathlib import Path
from typing import List, Dict, Tuple
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer
from github import Github
from github.PullRequest import PullRequest

def get_changed_files(pr: PullRequest) -> List[str]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Å–ø–∏—Å–æ–∫ –∏–∑–º–µ–Ω–µ–Ω–Ω—ã—Ö —Ñ–∞–π–ª–æ–≤ –≤ PR."""
    return [file.filename for file in pr.get_files()]

def run_ruff_check(file_path: str) -> Tuple[str, bool]:
    """–ó–∞–ø—É—Å–∫–∞–µ—Ç ruff –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ñ–∞–π–ª–∞."""
    try:
        result = subprocess.run(
            ["ruff", "check", file_path],
            capture_output=True,
            text=True
        )
        has_issues = bool(result.stdout.strip())
        return result.stdout, has_issues
    except subprocess.CalledProcessError as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–ø—É—Å–∫–µ ruff: {e}", True

def get_ai_recommendations(file_path: str, content: str, model, tokenizer) -> Tuple[str, bool]:
    """–ü–æ–ª—É—á–∞–µ—Ç —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –æ—Ç CodeLlama –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–æ–¥–∞."""
    try:
        prompt = f"""<s>[INST] –¢—ã - –æ–ø—ã—Ç–Ω—ã–π Python —Ä–∞–∑—Ä–∞–±–æ—Ç—á–∏–∫. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –∫–æ–¥ –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é. 
–§–æ–∫—É—Å–∏—Ä—É–π—Å—è –Ω–∞: —á–∏—Ç–∞–µ–º–æ—Å—Ç–∏, –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏, –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏ –∏ –ª—É—á—à–∏—Ö –ø—Ä–∞–∫—Ç–∏–∫–∞—Ö.

–ö–æ–¥ –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞:
```python
{content}
```

–î–∞–π –ø–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é –∫–æ–¥–∞. [/INST]"""

        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
        outputs = model.generate(
            **inputs,
            max_new_tokens=500,
            temperature=0.7,
            top_p=0.9,
            do_sample=True
        )
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ç–æ–ª—å–∫–æ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ (–ø–æ—Å–ª–µ –ø—Ä–æ–º–ø—Ç–∞)
        recommendations = response.split("[/INST]")[-1].strip()
        has_recommendations = bool(recommendations.strip())
        return recommendations, has_recommendations
    except Exception as e:
        return f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ AI —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π: {e}", True

def create_review_comment(pr: PullRequest, file_path: str, ruff_output: str, ai_recommendations: str):
    """–°–æ–∑–¥–∞–µ—Ç –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –∫ PR —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø—Ä–æ–≤–µ—Ä–∫–∏."""
    comment = f"""## –ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {file_path}

### –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ ruff:
```
{ruff_output}
```

### –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏ –ø–æ —É–ª—É—á—à–µ–Ω–∏—é:
{ai_recommendations}
"""
    pr.create_issue_comment(comment)

def notify_author(pr: PullRequest, files_with_issues: List[str]):
    """–û—Ç–ø—Ä–∞–≤–ª—è–µ—Ç —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä—É PR –æ –Ω–∞–π–¥–µ–Ω–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º–∞—Ö."""
    if not files_with_issues:
        return

    author = pr.user.login
    files_list = "\n".join(f"- {file}" for file in files_with_issues)
    
    notification = f"""–ü—Ä–∏–≤–µ—Ç, @{author}! üëã

–Ø –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–ª –≤–∞—à PR –∏ –Ω–∞—à–µ–ª –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–æ–º–µ–Ω—Ç–æ–≤, –∫–æ—Ç–æ—Ä—ã–µ —Å—Ç–æ–∏—Ç —É–ª—É—á—à–∏—Ç—å:

{files_list}

–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –æ–∑–Ω–∞–∫–æ–º—å—Ç–µ—Å—å —Å –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏—è–º–∏ –∫ –∫–∞–∂–¥–æ–º—É —Ñ–∞–π–ª—É –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è –ø–æ–¥—Ä–æ–±–Ω—ã—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–π.

–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã –∏–ª–∏ –Ω—É–∂–Ω–∞ –ø–æ–º–æ—â—å —Å –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è–º–∏, –¥–∞–π—Ç–µ –∑–Ω–∞—Ç—å! ü§ù
"""
    
    pr.create_issue_comment(notification)

def main():
    # –ü–æ–ª—É—á–∞–µ–º —Ç–æ–∫–µ–Ω –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
    github_token = os.getenv("GITHUB_TOKEN")
    
    if not github_token:
        print("–û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω GitHub —Ç–æ–∫–µ–Ω")
        sys.exit(1)
    
    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º GitHub –∫–ª–∏–µ–Ω—Ç
    g = Github(github_token)
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ PR
    repo_name = os.getenv("GITHUB_REPOSITORY")
    event_path = os.getenv("GITHUB_EVENT_PATH")
    
    if not event_path:
        print("–û—à–∏–±–∫–∞: –ù–µ –Ω–∞–π–¥–µ–Ω –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É —Å–æ–±—ã—Ç–∏—è")
        sys.exit(1)
        
    # –ß–∏—Ç–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ PR –∏–∑ —Ñ–∞–π–ª–∞ —Å–æ–±—ã—Ç–∏—è
    import json
    with open(event_path, 'r') as f:
        event_data = json.load(f)
        pr_number = event_data['pull_request']['number']
    
    print(f"–ê–Ω–∞–ª–∏–∑ PR #{pr_number}")
    
    repo = g.get_repo(repo_name)
    pr = repo.get_pull(pr_number)
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å CodeLlama
    print("–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ CodeLlama...")
    model_name = "codellama/CodeLlama-7b-Instruct-hf"
    
    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø—É—Ç–∏
    cache_dir = os.getenv("TRANSFORMERS_CACHE", "~/.cache/huggingface/hub")
    torch_cache_dir = os.getenv("TORCH_HOME", "~/.cache/torch")
    
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à –º–æ–¥–µ–ª–µ–π: {cache_dir}")
    print(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à PyTorch: {torch_cache_dir}")
    
    tokenizer = AutoTokenizer.from_pretrained(
        model_name,
        trust_remote_code=True,
        cache_dir=cache_dir
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        device_map="auto",
        trust_remote_code=True,
        cache_dir=cache_dir,
        torch_dtype=torch.float16  # –ò—Å–ø–æ–ª—å–∑—É–µ–º float16 –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –ø–∞–º—è—Ç–∏
    ).eval()
    
    # –ü–æ–ª—É—á–∞–µ–º –∏–∑–º–µ–Ω–µ–Ω–Ω—ã–µ —Ñ–∞–π–ª—ã
    changed_files = get_changed_files(pr)
    files_with_issues = []
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∫–∞–∂–¥—ã–π —Ñ–∞–π–ª
    for file_path in changed_files:
        if not file_path.endswith('.py'):
            continue
            
        print(f"–ê–Ω–∞–ª–∏–∑ —Ñ–∞–π–ª–∞: {file_path}")
        
        # –ü–æ–ª—É—á–∞–µ–º —Å–æ–¥–µ—Ä–∂–∏–º–æ–µ —Ñ–∞–π–ª–∞
        file_content = repo.get_contents(file_path, ref=pr.head.sha).decoded_content.decode()
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –ø—Ä–æ–≤–µ—Ä–∫–∏
        ruff_output, has_ruff_issues = run_ruff_check(file_path)
        ai_recommendations, has_ai_recommendations = get_ai_recommendations(file_path, file_content, model, tokenizer)
        
        # –ï—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã, –¥–æ–±–∞–≤–ª—è–µ–º —Ñ–∞–π–ª –≤ —Å–ø–∏—Å–æ–∫
        if has_ruff_issues or has_ai_recommendations:
            files_with_issues.append(file_path)
        
        # –°–æ–∑–¥–∞–µ–º –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
        create_review_comment(pr, file_path, ruff_output, ai_recommendations)
    
    # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ –∞–≤—Ç–æ—Ä—É, –µ—Å–ª–∏ –µ—Å—Ç—å –ø—Ä–æ–±–ª–µ–º—ã
    if files_with_issues:
        notify_author(pr, files_with_issues)

if __name__ == "__main__":
    main() 